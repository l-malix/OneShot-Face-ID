{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IMPORTATIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import cv2\n",
    "import uuid\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DATASET CONSTRUCTION AND PREPROCESSING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Path configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "\n",
    "ANCHORS_PATH = os.path.join(\"data\", \"anchors\")\n",
    "NEGATIVES_PATH = os.path.join(\"data\", \"negatives\")\n",
    "POSITIVES_PATH = os.path.join(\"data\", \"positives\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Make directories if they're is not exist\n",
    "\n",
    "if not os.path.exists(ANCHORS_PATH):\n",
    "    os.makedirs(ANCHORS_PATH)\n",
    "\n",
    "if not os.path.exists(NEGATIVES_PATH):\n",
    "    os.makedirs(NEGATIVES_PATH)\n",
    "\n",
    "if not os.path.exists(POSITIVES_PATH):\n",
    "    os.makedirs(POSITIVES_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Extract the data from lfw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! tar -xvzf lfw.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Move files from lfw to data/negatives "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dir in os.listdir(\"lfw\"):\n",
    "    for image_dir in os.listdir(os.path.join(\"lfw\", dir)):\n",
    "        old_path = os.path.join(os.path.join(\"lfw\", dir), image_dir)\n",
    "        os.replace(old_path, os.path.join(NEGATIVES_PATH, image_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Collect Anchors & Positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "cam = cv2.VideoCapture(0)\n",
    "while cam.isOpened():\n",
    "    _, frame = cam.read()\n",
    "    frame = frame[120:120+250, 200:200+250, :]\n",
    "    cv2.imshow(\"image collection\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0XFF == ord(\"p\"):\n",
    "        image_name = \"{}.jpg\".format(uuid.uuid1())\n",
    "        cv2.imwrite(os.path.join(POSITIVES_PATH, image_name), frame)\n",
    "\n",
    "    elif cv2.waitKey(1) & 0XFF == ord(\"a\"):\n",
    "        image_name = \"{}.jpg\".format(uuid.uuid1())\n",
    "        cv2.imwrite(os.path.join(ANCHORS_PATH, image_name), frame)\n",
    "\n",
    "    elif cv2.waitKey(1) & 0XFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cam.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make a dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "anchors = tf.data.Dataset.list_files(os.path.join(ANCHORS_PATH,\"*.jpg\")).take(400)\n",
    "positives = tf.data.Dataset.list_files(os.path.join(POSITIVES_PATH,\"*.jpg\")).take(400)\n",
    "negatives = tf.data.Dataset.list_files(os.path.join(NEGATIVES_PATH,\"*.jpg\")).take(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_image(image_path):\n",
    "    img = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_jpeg(img)\n",
    "    img = tf.image.resize(img, (100, 100))\n",
    "    img = img / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Building a tensorflow dataset\n",
    "\n",
    "positive = tf.data.Dataset.zip((anchors, positives, tf.data.Dataset.from_tensor_slices(tf.ones(len(anchors)))))\n",
    "negative = tf.data.Dataset.zip((anchors, negatives, tf.data.Dataset.from_tensor_slices(tf.zeros(len(anchors)))))\n",
    "dataset = positive.concatenate(negative)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(input, compared, label):\n",
    "    input_image = preprocess_image(input)\n",
    "    compared_image = preprocess_image(compared)\n",
    "    return input_image, compared_image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the dataloader\n",
    "\n",
    "dataset = dataset.map(preprocess_data)\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data preparation (70% of the data)\n",
    "train_dataset = dataset.take(round(len(dataset)*0.7))\n",
    "train_dataset = train_dataset.batch(16)\n",
    "train_dataset = train_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data preparation (30% of the data)\n",
    "test_dataset = dataset.skip(round(len(dataset)*0.7))\n",
    "test_dataset = test_dataset.batch(16)\n",
    "test_dataset = test_dataset.prefetch(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MODEL ARCHITECTURE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The encoder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder():\n",
    "    input = keras.layers.Input(shape=(100,100,3))\n",
    "\n",
    "    conv1 = keras.layers.Conv2D(64, (10,10), activation=\"relu\")(input)\n",
    "    pool1 = keras.layers.MaxPooling2D(2)(conv1)\n",
    "\n",
    "    conv2 = keras.layers.Conv2D(128, (7,7), activation=\"relu\")(pool1)\n",
    "    pool2 = keras.layers.MaxPooling2D(2)(conv2)\n",
    "\n",
    "    conv3 = keras.layers.Conv2D(128, (4,4), activation=\"relu\")(pool2)\n",
    "    pool3 = keras.layers.MaxPooling2D(2)(conv3)\n",
    "\n",
    "    conv4 = keras.layers.Conv2D(256, (4,4), activation=\"relu\")(pool3)\n",
    "    flatten = keras.layers.Flatten()(conv4)\n",
    "\n",
    "    dense = keras.layers.Dense(4096, activation=\"sigmoid\")(flatten)\n",
    "\n",
    "    return keras.models.Model(inputs=[input], outputs=[dense], name=\"Encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"Encoder\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_8 (InputLayer)        [(None, 100, 100, 3)]     0         \n",
      "                                                                 \n",
      " conv2d_21 (Conv2D)          (None, 91, 91, 64)        19264     \n",
      "                                                                 \n",
      " max_pooling2d_17 (MaxPoolin  (None, 45, 45, 64)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_22 (Conv2D)          (None, 39, 39, 128)       401536    \n",
      "                                                                 \n",
      " max_pooling2d_18 (MaxPoolin  (None, 19, 19, 128)      0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_23 (Conv2D)          (None, 16, 16, 128)       262272    \n",
      "                                                                 \n",
      " max_pooling2d_19 (MaxPoolin  (None, 8, 8, 128)        0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " conv2d_24 (Conv2D)          (None, 5, 5, 256)         524544    \n",
      "                                                                 \n",
      " flatten_4 (Flatten)         (None, 6400)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 4096)              26218496  \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 27,426,112\n",
      "Trainable params: 27,426,112\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model = encoder()\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Make l_1 distance layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L1Dist(keras.layers.Layer):\n",
    "    def __init__(self, **kwargs) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "    def call(self, input_encoder, compared_encoder):\n",
    "        return tf.math.abs(input_encoder - compared_encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Siamese Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "def siamese():\n",
    "\n",
    "    input_image = keras.layers.Input(shape=(100,100,3), name=\"input_image\")\n",
    "\n",
    "    compare_image = keras.layers.Input(shape=(100,100,3), name=\"compare_image\")\n",
    "\n",
    "    siamese_layer = L1Dist()\n",
    "    siamese_layer._name = \"distance\"\n",
    "    distance = siamese_layer(encoder_model(input_image), encoder_model(compare_image))\n",
    "\n",
    "    output = keras.layers.Dense(1, activation=\"sigmoid\")(distance)\n",
    "\n",
    "    return keras.models.Model(inputs=[input_image, compare_image], outputs=[output], name=\"siamese\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"siamese\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_image (InputLayer)       [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " compare_image (InputLayer)     [(None, 100, 100, 3  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " Encoder (Functional)           (None, 4096)         27426112    ['input_image[0][0]',            \n",
      "                                                                  'compare_image[0][0]']          \n",
      "                                                                                                  \n",
      " distance (L1Dist)              (None, 4096)         0           ['Encoder[0][0]',                \n",
      "                                                                  'Encoder[1][0]']                \n",
      "                                                                                                  \n",
      " dense_6 (Dense)                (None, 1)            4097        ['distance[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 27,430,209\n",
      "Trainable params: 27,430,209\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "siamese_model = siamese()\n",
    "siamese_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TRAINING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce_loss = tf.losses.BinaryCrossentropy()\n",
    "optim = tf.optimizers.Adam(learning_rate=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Establish a checkpoint rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(opt=optim, siamese_model=siamese_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(batch):\n",
    "    \n",
    "    # Record all of our operations \n",
    "    with tf.GradientTape() as tape:     \n",
    "        # Get anchor and positive/negative image\n",
    "        X = batch[:2]\n",
    "        # Get label\n",
    "        y = batch[2]\n",
    "        \n",
    "        # Forward pass\n",
    "        yhat = siamese_model(X, training=True)\n",
    "        # Calculate loss\n",
    "        loss = bce_loss(y, yhat)\n",
    "    print(loss)\n",
    "        \n",
    "    # Calculate gradients\n",
    "    grad = tape.gradient(loss, siamese_model.trainable_variables)\n",
    "    \n",
    "    # Calculate updated weights and apply to siamese model\n",
    "    optim.apply_gradients(zip(grad, siamese_model.trainable_variables))\n",
    "    \n",
    "    # Return loss\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(data, EPOCHS):\n",
    "\n",
    "    # Loop through epochs\n",
    "    for epoch in range(1, EPOCHS+1):\n",
    "        total_loss = 0\n",
    "        \n",
    "        progbar = tf.keras.utils.Progbar(len(data))\n",
    "        \n",
    "        # Loop through each batch\n",
    "        for idx, batch in enumerate(data):\n",
    "            # Run train step here\n",
    "            loss = train_step(batch)\n",
    "            total_loss += loss.numpy()\n",
    "            progbar.update(idx+1)\n",
    "        \n",
    "        print('\\n Epoch {}/{} Loss : {}'.format(epoch, EPOCHS, total_loss / len(data)))\n",
    "        losses.append(total_loss/len(data))\n",
    "        # Save checkpoints\n",
    "        if epoch % 10 == 0: \n",
    "            checkpoint.save(file_prefix=checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35/35 [==============================] - 114s 3s/step\n",
      "\n",
      " Epoch 1/50 Loss : 0.03783444711672408\n",
      "35/35 [==============================] - 102s 3s/step\n",
      "\n",
      " Epoch 2/50 Loss : 0.017098063127403813\n",
      "35/35 [==============================] - 104s 3s/step\n",
      "\n",
      " Epoch 3/50 Loss : 0.01125013724834259\n",
      "35/35 [==============================] - 99s 3s/step\n",
      "\n",
      " Epoch 4/50 Loss : 0.004813928767440042\n",
      "35/35 [==============================] - 106s 3s/step\n",
      "\n",
      " Epoch 5/50 Loss : 0.0046395606328068035\n",
      "35/35 [==============================] - 114s 3s/step\n",
      "\n",
      " Epoch 6/50 Loss : 0.05288344306339111\n",
      "35/35 [==============================] - 106s 3s/step\n",
      "\n",
      " Epoch 7/50 Loss : 0.03012679327013237\n",
      "35/35 [==============================] - 102s 3s/step\n",
      "\n",
      " Epoch 8/50 Loss : 0.020815464834283506\n",
      "35/35 [==============================] - 116s 3s/step\n",
      "\n",
      " Epoch 9/50 Loss : 0.008488680538721382\n",
      "35/35 [==============================] - 108s 3s/step\n",
      "\n",
      " Epoch 10/50 Loss : 0.002260158098527297\n",
      "35/35 [==============================] - 123s 4s/step\n",
      "\n",
      " Epoch 11/50 Loss : 0.0016671549686829426\n",
      "35/35 [==============================] - 103s 3s/step\n",
      "\n",
      " Epoch 12/50 Loss : 0.0014080174543778412\n",
      "35/35 [==============================] - 117s 3s/step\n",
      "\n",
      " Epoch 13/50 Loss : 0.0012354018391176527\n",
      "35/35 [==============================] - 118s 3s/step\n",
      "\n",
      " Epoch 14/50 Loss : 0.0010833671071200765\n",
      "35/35 [==============================] - 105s 3s/step\n",
      "\n",
      " Epoch 15/50 Loss : 0.0008705824765326854\n",
      "35/35 [==============================] - 87s 2s/step\n",
      "\n",
      " Epoch 16/50 Loss : 0.0007032859155775181\n",
      "35/35 [==============================] - 82s 2s/step\n",
      "\n",
      " Epoch 17/50 Loss : 0.0005334901486224096\n",
      "35/35 [==============================] - 107s 3s/step\n",
      "\n",
      " Epoch 18/50 Loss : 0.0004060261953522318\n",
      "35/35 [==============================] - 105s 3s/step\n",
      "\n",
      " Epoch 19/50 Loss : 0.00042349390480792085\n",
      "35/35 [==============================] - 95s 3s/step\n",
      "\n",
      " Epoch 20/50 Loss : 0.00036497454010324355\n",
      "35/35 [==============================] - 101s 3s/step\n",
      "\n",
      " Epoch 21/50 Loss : 0.000325289133356169\n",
      "35/35 [==============================] - 84s 2s/step\n",
      "\n",
      " Epoch 22/50 Loss : 0.00027317180188869575\n",
      "35/35 [==============================] - 82s 2s/step\n",
      "\n",
      " Epoch 23/50 Loss : 0.00035105285060126333\n",
      "35/35 [==============================] - 78s 2s/step\n",
      "\n",
      " Epoch 24/50 Loss : 0.0002677760987093539\n",
      "35/35 [==============================] - 78s 2s/step\n",
      "\n",
      " Epoch 25/50 Loss : 0.00024647909064827087\n",
      "35/35 [==============================] - 78s 2s/step\n",
      "\n",
      " Epoch 26/50 Loss : 0.000221104801182394\n",
      "35/35 [==============================] - 94s 3s/step\n",
      "\n",
      " Epoch 27/50 Loss : 0.00017474319421515767\n",
      "35/35 [==============================] - 93s 3s/step\n",
      "\n",
      " Epoch 28/50 Loss : 0.00018848812116110432\n",
      "35/35 [==============================] - 95s 3s/step\n",
      "\n",
      " Epoch 29/50 Loss : 0.0001503977082783032\n",
      "35/35 [==============================] - 96s 3s/step\n",
      "\n",
      " Epoch 30/50 Loss : 0.0001545385071689712\n",
      "35/35 [==============================] - 112s 3s/step\n",
      "\n",
      " Epoch 31/50 Loss : 0.00014299877756716991\n",
      "35/35 [==============================] - 101s 3s/step\n",
      "\n",
      " Epoch 32/50 Loss : 0.00012413675264854516\n",
      "35/35 [==============================] - 104s 3s/step\n",
      "\n",
      " Epoch 33/50 Loss : 0.0001222365884521943\n",
      "35/35 [==============================] - 102s 3s/step\n",
      "\n",
      " Epoch 34/50 Loss : 0.00010969650640747983\n",
      "35/35 [==============================] - 103s 3s/step\n",
      "\n",
      " Epoch 35/50 Loss : 7.523691604092165e-05\n",
      "35/35 [==============================] - 104s 3s/step\n",
      "\n",
      " Epoch 36/50 Loss : 8.348650392235868e-05\n",
      "35/35 [==============================] - 103s 3s/step\n",
      "\n",
      " Epoch 37/50 Loss : 8.024466847668269e-05\n",
      "35/35 [==============================] - 103s 3s/step\n",
      "\n",
      " Epoch 38/50 Loss : 6.641111185672344e-05\n",
      "35/35 [==============================] - 105s 3s/step\n",
      "\n",
      " Epoch 39/50 Loss : 6.58206018670171e-05\n",
      "35/35 [==============================] - 115s 3s/step\n",
      "\n",
      " Epoch 40/50 Loss : 6.287594159662799e-05\n",
      "35/35 [==============================] - 124s 4s/step\n",
      "\n",
      " Epoch 41/50 Loss : 7.176283679655171e-05\n",
      "35/35 [==============================] - 119s 3s/step\n",
      "\n",
      " Epoch 42/50 Loss : 6.469369336303186e-05\n",
      "35/35 [==============================] - 102s 3s/step\n",
      "\n",
      " Epoch 43/50 Loss : 5.2081084001136855e-05\n",
      "35/35 [==============================] - 98s 3s/step\n",
      "\n",
      " Epoch 44/50 Loss : 5.1376653813609404e-05\n",
      "35/35 [==============================] - 99s 3s/step\n",
      "\n",
      " Epoch 45/50 Loss : 4.885073277429391e-05\n",
      "35/35 [==============================] - 100s 3s/step\n",
      "\n",
      " Epoch 46/50 Loss : 4.868807048816442e-05\n",
      "35/35 [==============================] - 96s 3s/step\n",
      "\n",
      " Epoch 47/50 Loss : 3.975309562390196e-05\n",
      "35/35 [==============================] - 97s 3s/step\n",
      "\n",
      " Epoch 48/50 Loss : 3.5144204148959294e-05\n",
      "35/35 [==============================] - 97s 3s/step\n",
      "\n",
      " Epoch 49/50 Loss : 4.068376021807905e-05\n",
      "35/35 [==============================] - 96s 3s/step\n",
      "\n",
      " Epoch 50/50 Loss : 3.646665502076628e-05\n"
     ]
    }
   ],
   "source": [
    "## Start training\n",
    "EPOCHS = 50\n",
    "train(train_dataset, EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x208b6174dc8>]"
      ]
     },
     "execution_count": 268,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.metrics import Precision, Recall\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_image, compare_image, label = test_dataset.as_numpy_iterator().next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(data):\n",
    "    y_hats = []\n",
    "    ys = []\n",
    "\n",
    "    progbar = tf.keras.utils.Progbar(len(data))\n",
    "\n",
    "    for i, batch in enumerate(data):\n",
    "        \n",
    "        X = batch[:2]\n",
    "        y = batch[2]\n",
    "\n",
    "        y_hat = siamese_model(X)\n",
    "\n",
    "        y_hat = np.array([round(x[0]) for x in y_hat.numpy()])\n",
    "        \n",
    "        y_hats = np.concatenate((y_hats, y_hat))\n",
    "        ys = np.concatenate((ys, y.numpy()))\n",
    "\n",
    "        progbar.update(i+1)\n",
    "    \n",
    "    return y_hats, ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15/15 [==============================] - 12s 807ms/step\n"
     ]
    }
   ],
   "source": [
    "y_hat, y = eval(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Recall()\n",
    "m.update_state(y, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = Precision()\n",
    "m.update_state(y, y_hat)\n",
    "m.result().numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAVE & RELOAD THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "siamese_model.save(\"siamese_model.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "loaded_model = keras.models.load_model(\"siamese_model.h5\", \n",
    "                                    custom_objects={\"L1Dist\" : L1Dist, \"BinaryCrossentropy\":tf.losses.BinaryCrossentropy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[3.3785675e-06],\n",
       "       [9.7394137e-10],\n",
       "       [4.9010650e-06],\n",
       "       [6.2687189e-10],\n",
       "       [9.9988425e-01],\n",
       "       [1.0000000e+00],\n",
       "       [1.8692150e-06],\n",
       "       [9.9987352e-01],\n",
       "       [9.9999511e-01],\n",
       "       [1.0000000e+00],\n",
       "       [9.9999988e-01],\n",
       "       [1.3601350e-09],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [1.0000000e+00],\n",
       "       [9.9994421e-01]], dtype=float32)"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.predict([input_image, compare_image])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "77153cfac9044db7c2021ed41dc385779916320afa63e17e70d27a812b0e919a"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 64-bit ('deep_tf': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
